{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m1pdNqp4yRYm",
        "SyoF00_2yhpP"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voCDtGT9woTT"
      },
      "source": [
        "# **Multi-couches**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIzYRHo1fes"
      },
      "source": [
        "### Import des librairies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaTL7SK31TDh"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErfKh8qw2LTc"
      },
      "source": [
        "### Charger les données MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Erbhdwv52ICd"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# réduire la taille du train pour accélérer les experiences\n",
        "nnn=1000\n",
        "x_train = x_train[:nnn]\n",
        "y_train = y_train[:nnn]\n",
        "\n",
        "print(\"x_train\",x_train.shape)\n",
        "print(\"Nombre d'image pour entrainer:\",x_train.shape[0])\n",
        "print(\"Nombre d'image pour tester:\",x_test.shape[0])\n",
        "print(\"La taille d'une image:\",x_train.shape[1:],\"ce qui fait au total:\",\n",
        "      x_train.shape[1]*x_train.shape[2],\"pixels\")\n",
        "print(\"La liste des classes:\",np.unique(y_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6mSpG2n3cYG"
      },
      "source": [
        "### Définition d'une fonction qui transforme en représentation binaire pour plusieurs classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmM7LYZ3iGB"
      },
      "source": [
        "def transform_labels(y_train,y_test):\n",
        "  \"\"\"\n",
        "  Cette fonction transforme les classes non-binaires en une représentation binaire\n",
        "  Par exemple si on a une liste de 6 fleurs chacune peut avoir une des 3 classes\n",
        "  Entrée: [\n",
        "           1,\n",
        "           3,\n",
        "           3,\n",
        "           2,\n",
        "           1,\n",
        "           2\n",
        "          ]\n",
        "\n",
        "  Sortie: [\n",
        "           [1,0,0], # class 1\n",
        "           [0,0,1], # class 3\n",
        "           [0,0,1], # class 3\n",
        "           [0,1,0], # class 2\n",
        "           [1,0,0], # class 1\n",
        "           [0,1,0]  # class 2\n",
        "          ]\n",
        "  \"\"\"\n",
        "\n",
        "  print('y_train',y_train.shape)\n",
        "  print('y_test',y_test.shape)\n",
        "\n",
        "  # concatener train et test\n",
        "  y_train_test = np.concatenate((y_train,y_test),axis =0)\n",
        "\n",
        "  # init un encoder Label\n",
        "  encoder = LabelEncoder()\n",
        "  # transformer de [1,3,3,2,1,2] à [0,2,2,1,0,1]\n",
        "  new_y_train_test = encoder.fit_transform(y_train_test)\n",
        "\n",
        "  # init un encoder one-hot\n",
        "  encoder = OneHotEncoder()\n",
        "  # transformer de [0,2,2,1,0,1] à la représentation binaire\n",
        "  new_y_train_test = encoder.fit_transform(new_y_train_test.reshape(-1,1))\n",
        "\n",
        "  # resplit the train and test\n",
        "  new_y_train = new_y_train_test[0:len(y_train)]\n",
        "  new_y_test = new_y_train_test[len(y_train):]\n",
        "\n",
        "  print('new_y_train',new_y_train.shape)\n",
        "  print('new_y_test',new_y_test.shape)\n",
        "\n",
        "  return new_y_train, new_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8OWHk9m3q3N"
      },
      "source": [
        "### Visualiser quelques images en spécifiant que c'est du noir et blanc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGB55mPt3yCV"
      },
      "source": [
        "plt.subplot(221)\n",
        "plt.imshow(x_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(222)\n",
        "plt.imshow(x_train[1], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(223)\n",
        "plt.imshow(x_train[2], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(224)\n",
        "plt.imshow(x_train[12], cmap=plt.get_cmap('gray'))\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lAQJnPi4MtA"
      },
      "source": [
        "## Entrainer un MLP : Perceptron Multi-Couches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnyEdvg44U_K"
      },
      "source": [
        "### Transformer les données de `(28,28)` à `(784,1)` - Aplatir l'image en vecteur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk0NkdPz4ST8"
      },
      "source": [
        "n_train = x_train.shape[0]\n",
        "x_train = x_train.reshape(n_train,784)\n",
        "\n",
        "n_test = x_test.shape[0]\n",
        "x_test = x_test.reshape(n_test,784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfg7LKCm-vlE"
      },
      "source": [
        "### Normaliser les données pour que chaque pixel soit entre 0 e 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD03rgme-0mW"
      },
      "source": [
        "x_train = normalize(x_train)\n",
        "x_test = normalize(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbdAIHQ06c9B"
      },
      "source": [
        "### Transformer les classes en représentation binaire (one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zshX64ia6k0h"
      },
      "source": [
        "y_train_binaire,y_test_binaire = transform_labels(y_train,y_test)\n",
        "y_train_binaire = y_train_binaire.toarray()\n",
        "y_test_binaire = y_test_binaire.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fQg5Y55Ih3"
      },
      "source": [
        "### Créer la couche d'entrée qui a la même shape que celle d'une instance dans `x_train`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zngLs4Ij5KRY"
      },
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "input_layer = keras.layers.Input(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQfP0pzv5MRl"
      },
      "source": [
        "### Créer une couche cachée qui contient 512 neurones qui prend en entré le input_layer avec une activation `tanh`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErNs_SF95VEm"
      },
      "source": [
        "# on utilise Dense car on veut que tous les neurones (de hidden_layer_1) soient connectés à toutes les sorties (de input_layer)\n",
        "hidden_layer_1 = keras.layers.Dense(units=512, activation='tanh')(input_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iat7sIWk54hh"
      },
      "source": [
        "### Créer une couche sortie qui contient un nombre de neurones égal au nombre des classes avec une activation `softmax` et qu'elle soit liée à `hidden_layer_1`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ4yDv5u6CLQ"
      },
      "source": [
        "nb_classes = y_train_binaire.shape[1]\n",
        "output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(hidden_layer_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVinLhG5ZBc"
      },
      "source": [
        "### Créer maintenant le modèle (qui est un MLP avec une couche cachée)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-JDfhZ56_MW"
      },
      "source": [
        "model = keras.models.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2XAtG_x9gO1"
      },
      "source": [
        "### Visualiser les informations du modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuWECD7A9jwc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXiC3XB_7Arw"
      },
      "source": [
        "### Choisir l'algorithme d'optimisation avec un learning rate de 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzwz04P47I6f"
      },
      "source": [
        "learning_rate = 0.1\n",
        "optimizer_algo = keras.optimizers.SGD(learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l3NCQkn7Ld5"
      },
      "source": [
        "### Choisir la fonction de coût que l'on veut optimiser: (Categorical Cross-Entropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBeUTJi27Ncg"
      },
      "source": [
        "cost_function = keras.losses.categorical_crossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiayqZSi7PH1"
      },
      "source": [
        "### Compiler le modèle en lui indiquant que l'on veut mesurer aussi l'accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tez6_k5z7XLK"
      },
      "source": [
        "model.compile(loss=cost_function,optimizer=optimizer_algo, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlnv2wtL1yzl"
      },
      "source": [
        "### Spécifier le fait qu'on veut sauvegarder le meilleur modèle sur le valdiation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvM1Ec4x18L1"
      },
      "source": [
        " model_checkpoint = keras.callbacks.ModelCheckpoint('best-model.h5', monitor='val_loss', save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41VBR7lm7chr"
      },
      "source": [
        "## Entrainement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dnurlv0z7d11"
      },
      "source": [
        "### Choisir le batch size et le nombre d'époques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYDCgfDS7icy"
      },
      "source": [
        "mini_batch_size = 256\n",
        "nb_epochs = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2lWrZzk7vJj"
      },
      "source": [
        "### Entrainer en lui spécifiant d'utiliser une partie du train pour la validation des hyper-paramèteres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZDLGjn174aP"
      },
      "source": [
        "percentage_of_train_as_validation = 0.3\n",
        "history = model.fit(x_train,y_train_binaire,batch_size=mini_batch_size,\n",
        "                    epochs=nb_epochs,verbose=False,\n",
        "                    validation_split=percentage_of_train_as_validation,\n",
        "                    callbacks=[model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpKPOwC79AZy"
      },
      "source": [
        "### Tracer la variation du taux d'erreur sur le train et sur le validation set en fonction du nombre d'epoque"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKQV5btX9Jtb"
      },
      "source": [
        "history_dict = history.history\n",
        "loss_train_epochs = history_dict['loss']\n",
        "loss_val_epochs = history_dict['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss_train_epochs,color='blue',label='train_loss')\n",
        "plt.plot(loss_val_epochs,color='red',label='val_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.savefig('epoch-loss.pdf')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQHxpTTT2KaC"
      },
      "source": [
        "### Choisir le modèle sauvegardé\n",
        "\n",
        "> Bloc en retrait\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dc5i5ar2JPt"
      },
      "source": [
        "model = keras.models.load_model('best-model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8-XYaZgDFrb"
      },
      "source": [
        "## **Exercices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Huwj8sk5E_bg"
      },
      "source": [
        "### Évaluer l'accuracy de ce meilleur modèle sur le test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOsoisSbyOXM"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1pdNqp4yRYm"
      },
      "source": [
        " ### Corrigé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp77oOTsFBC5"
      },
      "source": [
        "loss,acc = model.evaluate(x_test,y_test_binaire,verbose=False)\n",
        "\n",
        "print(\"L'accuracy sur l'ensemble du test est:\",acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnYl-imgDZF0"
      },
      "source": [
        "### Essayer d'améliorer le modèle en variant :\n",
        "\n",
        "\n",
        "1.   [L'algorithme d'optimisation](https://keras.io/optimizers/)\n",
        "2.   [Fonctions d'activation](https://keras.io/activations/)\n",
        "2.   [nombre de couches Dense](https://keras.io/layers/core/)\n",
        "3.   [Nombre de neurones dans les couches](https://keras.io/layers/core/)\n",
        "4.   [batch_size](https://keras.io/models/model/#fit)\n",
        "5.   [taux d'apprentissage](https://keras.io/optimizers)\n",
        "\n",
        "Créer une liste des valeurs (hyperparamètres) à essayer pour chaque type de variation.\n",
        "Choisir aléatoirement une combinaison de ces hyperparamèteres.\n",
        "Répéter vos essais pour trouver celle qui marche le mieux sur le validation set.\n",
        "Noter l'effet sur le coût de chaque variation d'hyperparamètres (par exemple un très grand learning rate peut mener à un overfitting - un petit nombre de neurones peut mener à un underfitting).\n",
        "Évaluer votre modèle final sur le test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8QMc16kygC5"
      },
      "source": [
        "# Votre code ici"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyoF00_2yhpP"
      },
      "source": [
        " ### Corrigé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5toeoNIF-Jg"
      },
      "source": [
        "# nombre de jeu d'hyperparamètres à essayer\n",
        "iteration_s = 10\n",
        "\n",
        "# une liste de learning rate à essayer\n",
        "learning_rate_s = [2.0, 1.0, 0.1, 0.01, 0.001]\n",
        "# une liste du batch size à essayer\n",
        "mini_batch_size_s = [8,16,32,64,128,256,512]\n",
        "\n",
        "# initialiser l'erreur minimum à l'infini\n",
        "min_loss = np.inf\n",
        "\n",
        "print('learning_rate,batch_size,train_loss,val_loss')\n",
        "\n",
        "# répéter pour un certain nombre d'iterations\n",
        "for iteration in range(iteration_s):\n",
        "\n",
        "  # récupérer aléatoirement des hyperparamèters à essayer\n",
        "  learning_rate =  random.choice(learning_rate_s)\n",
        "  mini_batch_size =  random.choice(mini_batch_size_s)\n",
        "\n",
        "  # créer la couche d'entrée qui a la même shape qu'un image dans x_train\n",
        "  input_shape = x_train.shape[1:]\n",
        "  input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "  # créer la couche cachée de 128 neurones avec l'activation tanh\n",
        "  # à noter que vous devez changer le nombre de neurones ainsi que\n",
        "  # l'activation tanh d'une facon dynamique similaire à batch_size etc.\n",
        "  # n'oubliez pas que la couche cachée doit être liée à la couche d'entrée\n",
        "  hidden_layer_1 = keras.layers.Dense(units=128, activation='tanh')(input_layer)\n",
        "\n",
        "  # créer la couche de sortie avec un nombre de neurones égale au nombre\n",
        "  # de classe ainsi que l'activation softmax qui ne peut pas changer dans ce cas\n",
        "  nb_classes = y_train_binaire.shape[1]\n",
        "  output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(hidden_layer_1)\n",
        "\n",
        "  # créer le modèle en spécifiant l'input et output\n",
        "  model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "  # choisir l'algorithme d'optimisation (qui doit aussi lui même\n",
        "  # être choisi d'une manière dynamique) en lui spécifiant le learning rate\n",
        "  optimizer_algo = keras.optimizers.SGD(lr=learning_rate)\n",
        "\n",
        "  # choisir la fonction de coût\n",
        "  cost_function = keras.losses.categorical_crossentropy\n",
        "\n",
        "  # compiler en lui spécifiant l'accuracy à observer\n",
        "  model.compile(loss=cost_function,optimizer=optimizer_algo, metrics=['accuracy'])\n",
        "\n",
        "  # créer le callback model_checkpoint qui se base sur le validation loss\n",
        "  model_checkpoint = keras.callbacks.ModelCheckpoint('best-model.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "  # nombre d'epoque\n",
        "  nb_epochs = 100\n",
        "\n",
        "  # Entrainer en lui spécifiant d'utiliser 30% du train comme validation set\n",
        "  percentage_of_train_as_validation = 0.3\n",
        "  history = model.fit(x_train,y_train_binaire,batch_size=mini_batch_size,\n",
        "                    epochs=nb_epochs,verbose=False,\n",
        "                    validation_split=percentage_of_train_as_validation,\n",
        "                    callbacks=[model_checkpoint])\n",
        "\n",
        "  # tracer la variation pour savoir s'il y a du overfitting/underfitting\n",
        "  history_dict = history.history\n",
        "  loss_train_epochs = history_dict['loss']\n",
        "  loss_val_epochs = history_dict['val_loss']\n",
        "  plt.figure()\n",
        "  plt.plot(loss_train_epochs,color='blue',label='train_loss')\n",
        "  plt.plot(loss_val_epochs,color='red',label='val_loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "  plt.savefig('epoch-loss.pdf')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  # val_loss\n",
        "  val_loss = np.array(loss_val_epochs).min()\n",
        "\n",
        "  # train_loss\n",
        "  train_loss = loss_train_epochs[np.array(loss_val_epochs).argmin()]\n",
        "\n",
        "  print(learning_rate,mini_batch_size,train_loss,val_loss)\n",
        "\n",
        "  # effacer la mémoir de keras\n",
        "  keras.backend.clear_session()\n",
        "\n",
        "  # arreter pour chaque iteration pour pouvoir visualiser\n",
        "  input(\"Press Enter to continue...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atDYCmlgyjdj"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}